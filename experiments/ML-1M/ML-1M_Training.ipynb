{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Copyright 2021 Siemens AG\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "Authors:\n",
    "Yinchong Yang <yinchong.yang@siemens.com>\n",
    "Florian Buettner <buettner.florian@siemens.com>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/gpflow/session_manager.py:31: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/gpflow/session_manager.py:31: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/gpflow/misc.py:25: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/gpflow/misc.py:25: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/gpflow/saver/coders.py:80: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/mogp-dev/lib/python3.6/site-packages/gpflow/saver/coders.py:80: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import gpflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "import scipy as sp\n",
    "from scipy.sparse import coo_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import *\n",
    "\n",
    "sys.path.append('../../mogp_decomposition/')\n",
    "from mwgp import GPD\n",
    "from data import load_movielens_data_1m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "tf.set_random_seed(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_triple_store = load_movielens_data_1m('../../data/ML-1M/') \n",
    "\n",
    "N = ml_triple_store.shape[0]  # number of triples\n",
    "\n",
    "# Generate new split: \n",
    "# write_out = open('./ml-1m_splits.pkl', 'wb')\n",
    "# pickle.dump(splits, write_out)\n",
    "# write_out.close()\n",
    "\n",
    "# Load existent split: \n",
    "read_in = open('../../data/ML-1M/splits.pkl', 'rb')\n",
    "splits = pickle.load(read_in)\n",
    "read_in.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training with all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cv_id in range(5): \n",
    "    # Use the current trunk as test set and the rest as training\n",
    "    te_ids = splits[cv_id]  \n",
    "    tr_ids = []\n",
    "    for i in range(len(splits)): \n",
    "        if i != cv_id: \n",
    "            tr_ids.append(splits[i])\n",
    "    tr_ids = np.concatenate(tr_ids)    \n",
    "    \n",
    "    # As usual in GP, rescaling the target to be zero mean\n",
    "    target_scaler = StandardScaler()\n",
    "    \n",
    "    # I, J are numbers of users and movies, respectively.  \n",
    "    # Our model can also handle 3 input sources, i.e., kernels, corresponding to \n",
    "    #  tensor decomposition. Since ML-1M is a matrix decomposition task, we set K to None.\n",
    "    I = ml_triple_store[:, 0].max()+1\n",
    "    J = ml_triple_store[:, 1].max()+1\n",
    "    K = None\n",
    "    \n",
    "    # Prepare the training and test data: \n",
    "    #  The user indices and movie indices are found in the first two columns in the \n",
    "    #  triple store. The training target is the rescaled third column, i.e. ratings. \n",
    "    X_tr = ml_triple_store[tr_ids][:, 0:2]\n",
    "    Y_tr = target_scaler.fit_transform(ml_triple_store[tr_ids, 3][:, None]).reshape(-1)\n",
    "\n",
    "    X_te = ml_triple_store[te_ids][:, 0:2]\n",
    "    Y_te = target_scaler.transform(ml_triple_store[te_ids, 3][:, None]).reshape(-1)    \n",
    "    \n",
    "    # We initialize the latent representations with principal components. \n",
    "    #  For that we first generate a full matrix from the triple store.\n",
    "    X_tr_coo = coo_matrix((Y_tr.reshape(-1), (X_tr[:, 0], X_tr[:, 1])), shape=(I, J))\n",
    "    X_tr_dense = X_tr_coo.todense()\n",
    "    #  We take the leading principal components. \n",
    "    pca_user = PCA(8)\n",
    "    pca_item = PCA(8)\n",
    "    user_pcs = pca_user.fit_transform(X_tr_dense)\n",
    "    item_pcs = pca_item.fit_transform(X_tr_dense.T)\n",
    "\n",
    "    # The dictionary of hyper parameters: \n",
    "    hyper_params = {'I':I, 'J':J, 'K':K,\n",
    "                    'emb_sizes': [8, 8],     # the size of the latent representations. \n",
    "                    'M': 128,                # the number of inducing point pairs.\n",
    "                    'emb_reg': 1e-4,         # l2 regularization on representation vectors.\n",
    "                    'batch_size': 2**16,     # mini batch sizeof training\n",
    "                    'obs_mean': Y_tr.mean(), # mean of target, which is actually 0.\n",
    "                    'lr': 1e-2}              # learning rate.\n",
    "    \n",
    "    # Initialize the model with hyper parameters: \n",
    "    gp_md = GPD(**hyper_params)\n",
    "\n",
    "    # Specify the path to save trained model\n",
    "    gp_md.save_path = './ml-1m_M=128_cv'+str(cv_id)+'/'\n",
    "    \n",
    "    # Build the model. \n",
    "    gp_md.build()\n",
    "    \n",
    "    # Option 1: using PCA to initialize the latent representations. \n",
    "    # Note: so far we do not yet make use of the principal components as initialization \n",
    "    #  of the latent representations. \n",
    "    #  In order to achieve that using our current implementation, which only supports random \n",
    "    #  initialization, we have to apply a small trick: \n",
    "    \n",
    "    # First we call the save() method of the class, which saves two objects:  \n",
    "    #  The first object consists of the GP hyper parameters. \n",
    "    #  The second object consists of the latentrepresentations. \n",
    "    gp_md.save()\n",
    "    \n",
    "    # Second, we replace the current random representation with the principal components \n",
    "    #  in the gp_md model object. \n",
    "    param0 = gp_md.get_weights_params()\n",
    "    param0[0] = user_pcs  \n",
    "    param0[1] = item_pcs\n",
    "    # Note this does not change the parameters in the model since get_weights_params() method\n",
    "    #  makes a copy of the weights. Therefore: \n",
    "    \n",
    "    # Third, we only overwrite the previously saved weights \"model_params.pkl\" with the\n",
    "    #  PCA initialization, while leaving the other object, the GP parameters unchanged. \n",
    "    with open('./ml-1m_M=128_cv'+str(cv_id)+'/model_params.pkl', 'wb') as f: \n",
    "        pickle.dump(param0, f)\n",
    "    # Finally, we re-load the entire model, with old GP parameters but updated PC as \n",
    "    #  initializations of the latent representations.  \n",
    "    gp_md.load_params()\n",
    "    \n",
    "    # Option 2: Alternatively, one could also simply use the random initialization by simply\n",
    "    #  ignoring everything after gp_md.build()\n",
    "        \n",
    "    # Now we can start training: \n",
    "    #  The third and fourth parameters are validation X and Y, which we ommit for now. \n",
    "    gp_md.train(X_tr, Y_tr, None, None, n_iter=500)\n",
    "    \n",
    "    # Save the model after training. \n",
    "    gp_md.save()\n",
    "    \n",
    "    # Also save the scaler for this specific split configuration, which we need for evaluation. \n",
    "    with open('./ml-1m_scaler_cv'+str(cv_id)+'.pkl', 'wb') as f: \n",
    "        pickle.dump(target_scaler, f)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mogp-dev",
   "language": "python",
   "name": "mogp-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
